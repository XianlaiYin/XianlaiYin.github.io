<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Learning Diary - WEEK 3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./WEEK4.html" rel="next">
<link href="./WEEK2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><strong>WEEK 3</strong></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Learning Diary</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/XianlaiYin/XianlaiYin.github.io" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><strong>Introduction</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK1.html" class="sidebar-item-text sidebar-link"><strong>WEEK 1</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK2.html" class="sidebar-item-text sidebar-link"><strong>WEEK 2</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK3.html" class="sidebar-item-text sidebar-link active"><strong>WEEK 3</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK4.html" class="sidebar-item-text sidebar-link"><strong>WEEK 4</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK5.html" class="sidebar-item-text sidebar-link"><strong>WEEK 5</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK6.html" class="sidebar-item-text sidebar-link"><strong>WEEK 6</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK7.html" class="sidebar-item-text sidebar-link"><strong>WEEK 7</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./WEEK8.html" class="sidebar-item-text sidebar-link"><strong>WEEK 8</strong></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link"><strong>References</strong></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#summary-lecture" id="toc-summary-lecture" class="nav-link active" data-scroll-target="#summary-lecture"><strong>1 Summary: lecture</strong></a>
  <ul class="collapse">
  <li><a href="#pre-knowledge" id="toc-pre-knowledge" class="nav-link" data-scroll-target="#pre-knowledge"><strong>1.1 Pre-knowledge</strong></a></li>
  <li><a href="#corrections" id="toc-corrections" class="nav-link" data-scroll-target="#corrections"><strong>1.2 Corrections</strong></a></li>
  <li><a href="#data-joining-and-enhancement" id="toc-data-joining-and-enhancement" class="nav-link" data-scroll-target="#data-joining-and-enhancement"><strong>1.3 Data joining and enhancement</strong></a></li>
  </ul></li>
  <li><a href="#summary-practical" id="toc-summary-practical" class="nav-link" data-scroll-target="#summary-practical"><strong>2 Summary: practical</strong></a></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application"><strong>3 Application</strong></a>
  <ul class="collapse">
  <li><a href="#applications-of-remote-sensing-image-enhancement" id="toc-applications-of-remote-sensing-image-enhancement" class="nav-link" data-scroll-target="#applications-of-remote-sensing-image-enhancement"><strong>2.1 Applications of remote sensing image enhancement</strong></a></li>
  <li><a href="#application-case" id="toc-application-case" class="nav-link" data-scroll-target="#application-case"><strong>2.2 Application case</strong></a></li>
  <li><a href="#case-comments" id="toc-case-comments" class="nav-link" data-scroll-target="#case-comments"><strong>3.3 Case comments</strong></a></li>
  </ul></li>
  <li><a href="#reflection" id="toc-reflection" class="nav-link" data-scroll-target="#reflection"><strong>4 Reflection</strong></a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/XianlaiYin/XianlaiYin.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><strong>WEEK 3</strong></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="remote-sensing-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="remote-sensing-data"><strong><em>Remote sensing data</em></strong></h4>
<blockquote class="blockquote">
<p>This is a learning diary of CASA0023 WEEK 3, the lecture presentation is <a href="https://andrewmaclachlan.github.io/CASA0023-lecture-3/#1">here</a>, and the practical material is <a href="https://andrewmaclachlan.github.io/CASA0023/3_corrections.html">here</a>.</p>
</blockquote>
</section>
<section id="summary-lecture" class="level2">
<h2 class="anchored" data-anchor-id="summary-lecture"><strong>1 Summary: lecture</strong></h2>
<p>This week focuses on the pre-processing of remote sensing data, including the correction, joining and enhancement of various types of remote sensing data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/WEEK3_Mindmap.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Mindmap of Week 3 Leacture</figcaption><p></p>
</figure>
</div>
<hr>
<section id="pre-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="pre-knowledge"><strong>1.1 Pre-knowledge</strong></h3>
<section id="different-sensors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="different-sensors"><strong>Different sensors</strong></h4>
<ul>
<li><code>MSS</code> (Multispectral Scanner)</li>
<li><code>RBV</code> (Return Beam Vidicon Camera)</li>
</ul>
</section>
<section id="push-broom-vs-whisk-broom" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="push-broom-vs-whisk-broom"><strong>Push broom vs Whisk broom</strong></h4>
<ul>
<li><code>Whisk broom</code> or spotlight or across track scanners: Mirror reflects light onto 1 detector - Landsat</li>
<li><code>Push broom</code> or along track scanners: several detectors that are pushed along - SPOT, Quickbird</li>
</ul>
</section>
</section>
<section id="corrections" class="level3">
<h3 class="anchored" data-anchor-id="corrections"><strong>1.2 Corrections</strong></h3>
<section id="regression" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="regression"><strong>Regression</strong></h4>
<p><span class="math display">\[
y_i=\beta_0+\beta_1x_i+\varepsilon_i
\]</span></p>
<ul>
<li><code>β0</code> is the intercept (the value of y when x = 0)</li>
<li><code>β1</code> the ‘slope’ the change in the value of y for a 1 unit change in the value of x</li>
<li><code>ϵi</code> is a random error term (positive or negative)- if you add all of the vertical differences between the blue line and all of the residuals, it should sum to 0</li>
<li>Any value of y along the blue line can be modeled using the corresponding value of x</li>
</ul>
</section>
<section id="geometric-correction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="geometric-correction"><strong>Geometric correction</strong></h4>
<p><strong><em>What leads to image distortions</em></strong></p>
<ul>
<li><code>View angle</code> (off-nadir)</li>
<li><code>Topography</code> (e.g.&nbsp;hills not flat ground)</li>
<li><code>Wind</code> (if from a plane)</li>
<li><code>Rotation of the eart</code>h (from satellite)</li>
</ul>
<p><strong><em>Geometric correction solution</em></strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Geometric-correction-procedures.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Geometric correction. Source: <a href="https://www.researchgate.net/figure/Geometric-correction-procedures_fig7_320710942">Abdul Basith</a></figcaption><p></p>
</figure>
</div>
<ol type="1">
<li><p>Identify Ground Control Points (GPS) to match known points in the image and a reference dataset</p></li>
<li><p>Take the coordinates and model them to give geometric transformation coefficients, linear regression with our distorted x or y as the dependent or independent</p>
<ul>
<li><code>Input to output (forward mapping)</code>: the issue with this is that we are modelling the rectified x and y which could fall anywhere on the gold standard map (e.g.&nbsp;not on a grid square or at a floating point)</li>
<li><code>Output to input (backward mapping)</code>: for every value in the output (gold standard) pixel we can get a value in the original input image. The images are distorted as so might not completely overlap. The goal is to match the distorted image with the gold standard image, so we want the pixels to line up</li>
</ul></li>
<li><p>Plot these and try to minimise the RMSE - Jensen sets a RMSE value of 0.5, typically might add more GCPs to reduce the RMSE (The model with the lowest RMSE will fit best)</p>
<ul>
<li><code>RMSE</code>: (observed - predicted (the residual))^2, sum them and divide by number of data points, square root that total</li>
<li><code>Resample methods</code>: Nearest Neighbor, Linear, Cubic, Cubic spline</li>
</ul></li>
</ol>
</section>
<section id="atmospheric-correction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="atmospheric-correction"><strong>Atmospheric correction</strong></h4>
<p><strong><em>Necessary and unnecessary atmospheric correction</em></strong></p>
<ul>
<li>Necessary
<ul>
<li>Biophysical parameters needed (e.g.&nbsp;temperature, leaf area index, NDVI)</li>
<li>Using spectral signatures through time and space</li>
</ul></li>
<li>Unnecessary
<ul>
<li>Classification of a single image</li>
<li>Independent classification of multi date imagery</li>
<li>Composite images (combining images)</li>
<li>Single dates or where training data extracted from all data</li>
</ul></li>
</ul>
<p><strong><em>Atmospheric correction in action</em></strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/haze_atmospheric_correction.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Atmospheric correction examples of three scenes (Bands 1, 2, and 3). Source: <a href="https://www.researchgate.net/figure/Atmospheric-correction-examples-of-three-scenes-Bands-1-2-and-3-The-first-row-shows_fig6_3202775">Liang et al.&nbsp;2001</a></figcaption><p></p>
</figure>
</div>
<ul>
<li><code>Absorption</code> and <code>scattering</code> create the haze = reduces contrast of image</li>
<li>Scattering = can create the “adjacency effect”, radiance from pixels nearby mixed into pixel of interest</li>
</ul>
<p><strong><em>Atmospheric correction types</em></strong></p>
<ul>
<li>Relative (to something)
<ul>
<li><code>Normalize</code>
<ul>
<li>Normalize intensities of different bands within a single image</li>
<li>Normalise intensities of bands from many dates to one date</li>
</ul></li>
<li><code>Dark object subtraction (DOS)</code> or <code>histogram adjustment</code>
<ul>
<li>Searches each band for the darkest value then subtracts that from each pixel</li>
<li>Landsat bands 1-3 (visible) have increased scattering vs longer wavelengths</li>
</ul></li>
<li><code>Psuedo-invariant Features (PIFs)</code>
<ul>
<li>Assume brightness pixels linearly related to a base image</li>
<li>Regression per band</li>
<li>Adjust the image based on the regression result</li>
<li>Here y is the value of our base. To get y we multiply our new date pixel (x) by the coefficient and add the intercept value</li>
<li>Apply this to the rest of the pixels</li>
</ul></li>
</ul></li>
<li>Absolute (definitive)
<ul>
<li>Method
<ul>
<li>Change digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet</li>
<li>We do this through atmospheric radiative transfer models and there are many to select from</li>
<li>However, nearly all assume atmospheric measurements are available which are used to “invert” the image radiance to scaled surface reflectance</li>
<li>The scattering and absorption information comes from atmopshierc radiative transfer code such as MODTRAN 4+ and the Second Simulation of the Satellite Signal in the Solar Spectrum (6S), which can now be used through python - called <code>Py6S</code></li>
</ul></li>
<li>Absolute Data requirements
<ul>
<li><code>An atmopsheric model</code> (summer, tropical): usually you can select from the tool</li>
<li><code>Local atmopsheric visibility</code>: from a weather station, like airports</li>
<li><code>Image altitude</code></li>
</ul></li>
<li>Absolute Tools
<ul>
<li><code>ACORN</code>: Atmopsehic CORection Now</li>
<li><code>FLAASH</code>: Fast Line of-sight Atmopsheric Analysis</li>
<li><code>QUAC</code>: Quick Atmopsheric Correction</li>
<li><code>ATCOR</code>: The ATmospheric CORrection program</li>
<li><code>SMAC</code>: Simplified Model for Atmospheric Correction (SMAC)</li>
</ul></li>
</ul></li>
<li>Empirical Line Correction
<ul>
<li>We can go and take measurements in situ using a field spectrometer, this does require measurements at the same time as the satellite overpass</li>
<li>Then use these measurements in linear regression against the satellite data raw digital number</li>
</ul></li>
</ul>
</section>
<section id="orthorectification-topographic-correction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="orthorectification-topographic-correction"><strong>Orthorectification / Topographic correction</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/orthorectification.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">A view captured from an oblique angle (for example, 25°, left) must be corrected for relief displacement caused by terrain to generate the orthorectified view (looking straight down, right). Orthoimagery is produced by calculating the nadir view for every pixel. Source: <a href="https://www.esri.com/about/newsroom/insider/what-is-orthorectified-imagery/">Esri Insider, 2016</a></figcaption><p></p>
</figure>
</div>
<p><strong><em>A subset of georectification</em></strong></p>
<ul>
<li><code>Georectification</code> = giving coordinates to an image</li>
<li><code>Orthorectification</code> = removing distortions… making the pixels viewed at nadir (straight down)</li>
</ul>
<p><strong><em>Requires</em></strong></p>
<ul>
<li><code>Sensor geometry</code></li>
<li><code>An elevation model</code></li>
</ul>
<p><strong><em>Software / formulas to do this</em></strong></p>
<ul>
<li>Jensen covers the following <code>formulas</code>: Cosine correction, Minnaert correction, Statistical Empirical correction, C Correction (advancing the Cosine)</li>
<li><code>Software</code>: QGIS, SAGA GIS, R package topocorr, R package RStoolbox</li>
</ul>
<p><strong><em>Solar location</em></strong></p>
<ul>
<li><code>Solar azimuth</code> = compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset</li>
<li><code>Solar zenith</code> = angle of local zenith (above the point on ground) and sun from vertical (90° - elevation)</li>
</ul>
</section>
<section id="radiometric-calibration" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="radiometric-calibration"><strong>Radiometric calibration</strong></h4>
<ul>
<li>Sensors capture image brightness and distributed as a <code>Digital Number (or DN)</code> - allows for efficient storage but has no units</li>
<li>Spectral radiance is the amount of light within a band from a sensor in the <code>field of view (FOV)</code>, it is independent of the sensor, measured in Watts (power or light here)</li>
<li>DN to spectral radiance = <code>radiometric calibration</code></li>
<li><code>Sensor calibration</code> = the relationship between Gain and Bias are usually provided but we can calcaulte them</li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Terms
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><code>Radiance</code>: refers to any radiation leaving the Earth (i.e.&nbsp;upwelling, toward the sensor
<ul>
<li>Might also be called Top of Atmosphere (TOA) radiance</li>
<li>How much light the instrument sees in meaningful units but still have effects of: Light source, atmosphere and surface material</li>
<li>We can remove the effects of the light source to generate Top of Atmosphere reflectance but usually this is combined within the radiance to reflectance step</li>
</ul></li>
<li><code>Irradiance</code>: is used to describe downwelling radiation reaching the Earth from the sun</li>
<li><code>Digital number (DN)</code>
<ul>
<li>Intensity of the electromagnetic radiation per pixel</li>
<li>Pixel values that aren’t calibrated and have no unit</li>
<li>Have light source</li>
<li>Effects of sensor + atmosphere + material</li>
<li>Values range from 0 - 255 (Lansat 5) = 8 bit or 0 - 65536 Landsat 8 (12 bit)</li>
</ul></li>
<li><code>Reflectance</code>: We need to account for atmospheric and illumination effects to create reflectance. BUT this typically doesn’t deal with shadows and directional effects (e.g.&nbsp;viewing angles) = apparent reflectance However, this is often called reflectance
<ul>
<li>Reflectance is a property of a material (e.g.&nbsp;reflectance of grass is a property of grass)</li>
<li>The issue with radiance is that is contains physical properties AND is dependent on the light source</li>
</ul></li>
<li><code>Hemispherical reflectance</code>: all of the light leaving the surface goes straight to the sensor (nothing is intercepted or at an angle)</li>
<li><code>Path radiance</code>: radiance reflected above the surface (e.g.&nbsp;scattering)</li>
<li><code>Atmospheric attenuation</code>: absorption of EMR due to materials in atmosphere (e.g.&nbsp;water vapour)</li>
<li><code>Local</code>: specific to pixel</li>
<li>`Neighbourhood: pixels within a range (nearby)</li>
</ul>
</div>
</div>
</section>
</section>
<section id="data-joining-and-enhancement" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="data-joining-and-enhancement"><strong>1.3 Data joining and enhancement</strong></h3>
<section id="joining-data-sets" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="joining-data-sets"><strong>Joining data sets</strong></h4>
<ul>
<li>This is termed “Mosaicking” in remote sensing - but it’s not much different to merging in GIS</li>
<li>In Remote Sensing we usually <code>feather</code> the images together</li>
<li>This creates a seamless mosaic or image(s)</li>
<li>The dividing line is termed the <code>seamline</code></li>
<li>We have a base image and “other” or second image</li>
</ul>
</section>
<section id="how-to-join-data-sets" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-to-join-data-sets"><strong>How to join data sets</strong></h4>
<ul>
<li><code>Standardization</code> (dividing the SR value by a maximum value per band) and <code>normalization</code> (divide the standarised value by the sum of values across all bands) applied to each image</li>
<li>Undertake further relative <code>radiometric normalization</code></li>
<li><code>Classify</code> each image alone</li>
<li><code>Calculate</code> other metrics from the image</li>
</ul>
</section>
<section id="image-enhancement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="image-enhancement"><strong>Image enhancement</strong></h4>
<p><strong><em>Contrast enhancement</em></strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/image_enhancements.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Contrast enhancement in QGIS. Source: <a href="https://www.sigterritoires.fr/index.php/en/image-classification-tutorial-with-qgis-2-2-images-enhancement/">Atilo Francois</a></figcaption><p></p>
</figure>
</div>
<ul>
<li><code>Minimum - Maximum</code></li>
<li><code>Percentage Linear</code> and <code>Standard Deviation</code></li>
<li><code>Piecewise Linear Contrast Stretch</code></li>
</ul>
<p><strong><em>Ratio</em></strong></p>
<ul>
<li><code>Band ratioing</code> means dividing the pixels in one band by the corresponding pixels in a second band.</li>
<li>Example: Normalized Burn Ratio = (NIR - SWIR) / (NIR + SWIR)
<ul>
<li>In Landsat 4-7, NBR = (Band 4 – Band 7) / (Band 4 + Band 7)</li>
<li>In Landsat 8-9, NBR = (Band 5 – Band 7) / (Band 5 + Band 7)</li>
</ul></li>
</ul>
<p><strong><em>Filtering</em></strong></p>
<ul>
<li><code>Low pass</code> or <code>low frequency</code> (averages the surrounding pixels)</li>
<li><code>High pass</code> or <code>high frequency</code> - enhance local variations</li>
<li><code>Edge enhancement</code></li>
</ul>
<p><strong><em>PCA (Principal Component)</em></strong></p>
<ul>
<li>Transform multi-spectral data into uncorrelated and smaller dataset</li>
<li>Has most of the original information</li>
<li>Reduces future computation “dimensionatliy reduction”</li>
<li>The first component will capture most of the variance within the dataset</li>
<li>In R this is from the RStoolbox packagerasterPCA()</li>
<li>PCA example, multi-date PCA - bands from both time points are combined into one image, then PCA</li>
</ul>
<p><strong><em>Texture</em></strong></p>
<ul>
<li>Images just use tonal (spectral) data not texture</li>
<li><code>Texture</code>: spatial variation of gray values</li>
<li><code>First order (occurrence)</code>: use counts or occurrences</li>
<li><code>Second order(co-occurrence)</code>: relationship between pixel pairs “a function of both the angular relationship and distance between two (or kernel) neighboring pixels”</li>
</ul>
<p><strong><em>Fusion</em></strong></p>
<p><code>Image fusion</code> is where data from multiple sensors / sources is fused together</p>
<ul>
<li>Pan sharpen</li>
<li>Data Fusion</li>
</ul>
</section>
</section>
</section>
<section id="summary-practical" class="level2">
<h2 class="anchored" data-anchor-id="summary-practical"><strong>2 Summary: practical</strong></h2>
<blockquote class="blockquote">
<p>This week’s practical will consist mainly of the following:</p>
<ul>
<li>Introduction and access to Landsat data</li>
<li>A deeper understanding of atmospheric correction and how it works in practice</li>
<li>Introduction to the principles of Radiance (or DN) to Reflectance</li>
<li>Practical exercises in image joining</li>
<li>A deep understanding of image enhancement and how it works in practice</li>
</ul>
</blockquote>
<p>For this week’s practical I chose to follow the guidance using Landsat data from Cape Town. I used R to do atmospheric corrections, merging, data enhancement and other operations on the data. Through practical, I gained a better understanding of remote sensing data pre-processing and was able to apply this to my workflow in R. Combined with the previous term’s CASA0005 course, I can now use R to analyse more types of data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/my_plot.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Texture of landsat data from Cape Town</figcaption><p></p>
</figure>
</div>
</section>
<section id="application" class="level2">
<h2 class="anchored" data-anchor-id="application"><strong>3 Application</strong></h2>
<blockquote class="blockquote">
<p>This week, the main focus was on the correction, joining and enhancement of remote sensing images, mostly applied to pre-processing before analysis, but remote sensing image enhancement has a wider range of applications.</p>
</blockquote>
<section id="applications-of-remote-sensing-image-enhancement" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-remote-sensing-image-enhancement"><strong>2.1 Applications of remote sensing image enhancement</strong></h3>
<p>Remote sensing image enhancement refers to the improvement of the quality and information of remote sensing images through some technical means to make them more suitable for human vision or subsequent analytical processing. Remote sensing image enhancement can be applied to the following areas:</p>
<p><code>Contrast enhancement</code>: By adjusting the grey level of a remotely sensing image, the contrast of the image is increased to make it clearer and brighter. Contrast enhancement can be done using methods such as histogram stretching, histogram equalisation and segmented linear stretching.</p>
<p><code>Band ratioing</code>: Extracts specific information from remote sensing images, such as vegetation indices, water indices, soil indices, etc., by calculating the ratio between different bands. Band ratios can be calculated using methods such as band arithmetic and band combination.</p>
<p><code>Filtering</code>: Remove noise or enhance edges and textures in an image by convolving the remote sensing image in the spatial or frequency domain. Filtering can use methods such as smoothing filtering, sharpening filtering and edge detection filtering.</p>
<p><code>Principal component analysis (PCA)</code>: By applying orthogonal transformations to multiple bands of remote sensing images, the main information in the image is extracted and the redundancy and correlation of the data is reduced. Principal component analysis can use statistical methods or methods such as wavelet transform.</p>
<p><code>Texture</code>: Describes the surface roughness or structural features in an image by calculating the grey scale variation within a region of the remotely sensed image. Texturing can use methods such as grey level co-occurrence matrix, grey level distance matrix, grey level gradient matrix, etc.</p>
<p><code>Fusion</code>: Increasing the spatial or spectral resolution of an image and increasing the amount of information in the image by combining remote sensing images from different sources or at different resolutions. Fusion can be done using methods such as HIS transform, wavelet transform, multi-resolution analysis, etc.</p>
</section>
<section id="application-case" class="level3">
<h3 class="anchored" data-anchor-id="application-case"><strong>2.2 Application case</strong></h3>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Sourse
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong><em>PCA‐based land‐use change detection and analysis using multitemporal and multisensor satellite data</em></strong> , <em>Sourse: <span class="citation" data-cites="deng_pcabased_2008">Deng et al. (<a href="references.html#ref-deng_pcabased_2008" role="doc-biblioref">2008</a>)</span></em></p>
</div>
</div>
<p>Remote‐sensing change detection based on multitemporal, multispectral, and multisensor imagery has been developed over several decades and provided timely and comprehensive information for planning and decision‐making. In practice, however, it is still difficult to select a suitable change‐detection method, especially in urban areas, because of the impacts of complex factors.</p>
<p>This paper presents a new method using <code>multitemporal</code> and <code>multisensor</code> data (SPOT‐5 and Landsat data) to detect land‐use changes in an urban environment based on <code>principal‐component analysis (PCA)</code> and <code>hybrid classification</code> methods. After geometric correction and radiometric normalization, PCA was used to enhance the change information from stacked multisensor data. Then, a hybrid classifier combining unsupervised and supervised classification was performed to identify and quantify land‐use changes. Finally, stratified random and user‐defined plots sampling methods were synthetically used to obtain total 966 reference points for accuracy assessment.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/tres_a_295182_o_f0002g.jpeg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Example of land‐use changes from cropland to urban land (Economic and Technological Development Zone). (a) Pan image of ETM (2000); (b) aerial photograph (2000); (c) RGB composition image of SPOT‐5 (2003); (d) RGB composition image of IKONOS (2003); (e)–(h) first four principal components.</figcaption><p></p>
</figure>
</div>
<p>Although errors and confusion exist, this method shows satisfying results with an overall accuracy to be 89.54% and 0.88 for the kappa coefficient. When compared with the post‐classification method, PCA‐based change detection also showed a better accuracy in terms of overall, producer’s, and user’s accuracy and kappa index. The results suggested that significant land‐use changes have occurred in Hangzhou City from 2000 to 2003, which may be related to rapid economy development and urban expansion. It is further indicated that most changes occurred in cropland areas due to urban encroachment.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/tres_a_295182_o_f0005g.jpeg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Land use and land‐use change detected using the PCA‐based approach.</figcaption><p></p>
</figure>
</div>
</section>
<section id="case-comments" class="level3">
<h3 class="anchored" data-anchor-id="case-comments"><strong>3.3 Case comments</strong></h3>
<p><strong><em>Advantages or contribution</em></strong></p>
<ul>
<li>Flexible classification strategies: The hybrid classification approach allows the authors to combine the accuracy of supervised classification methods while taking advantage of the automation of unsupervised classification methods. This flexibility allows the method to be better adapted to different types of land use change scenarios.</li>
<li>PCA-based feature extraction: By combining multi-temporal and multi-sensor data into a single Principal Component Analysis (PCA) model, the authors are able to extract more meaningful features that help distinguish between changing and non-changing areas. This approach has advantages in dealing with high-dimensional data and reducing data redundancy.</li>
<li>Correction and radiation normalisation: This paper corrected and radiation normalised the data prior to analysis, which helped to reduce errors due to sensor differences, variations in atmospheric conditions and land cover type, thus improving the accuracy and reliability of the results.</li>
</ul>
<p><strong><em>Disadvantages or potential</em></strong></p>
<ul>
<li>Limitations of Principal Component Analysis (PCA): While PCA is able to extract and enhance variation information from multi-sensor data, it may not be able to completely eliminate noise and other non-target factors. In addition, PCA may have limitations in handling non-linear data, which may lead to inaccurate detection results.</li>
<li>Limitations of sensor data: SPOT-5 and Landsat data were used in this paper. While these data sources are of value in land cover change detection, they may not capture the detailed information that finer spatial resolution and higher spectral resolution can provide. In addition, these data sources may be affected by issues such as cloud occlusion, atmospheric interference and timing inconsistencies.</li>
</ul>
</section>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection"><strong>4 Reflection</strong></h2>
<p>This week I learnt the principles and methods of correcting, joining and enhancing remotely sensed images and understood how to pre-process remotely sensed data. Although most of the data we acquired had already been pre-processed, this knowledge gave me a better understanding of how to apply remote sensing images to the actual analysis and what could cause errors and whether further processing was needed. In addition, the study of remote sensing image enhancement has taught me more about remote sensing data analysis methods and principles in a deeper way.</p>
<p>However, not all data pre-processing can be perfectly accurate, errors are allowed but need to be controlled according to the actual situation. Also, in practice, some R packages are no longer available due to version updates. This is a common problem in data analysis and can be solved by downloading a lower version of the package or replacing it.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-deng_pcabased_2008" class="csl-entry" role="doc-biblioentry">
Deng, J. S., K. Wang, Y. H. Deng, and G. J. Qi. 2008. <span>“<span>PCA</span>‐based Land‐use Change Detection and Analysis Using Multitemporal and Multisensor Satellite Data.”</span> <em>International Journal of Remote Sensing</em> 29 (16): 4823–38. <a href="https://doi.org/10.1080/01431160801950162">https://doi.org/10.1080/01431160801950162</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./WEEK2.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><strong>WEEK 2</strong></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./WEEK4.html" class="pagination-link">
        <span class="nav-page-text"><strong>WEEK 4</strong></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>