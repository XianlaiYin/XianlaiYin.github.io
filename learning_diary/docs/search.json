[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary",
    "section": "",
    "text": "Content\n\n\n\nWEEK\nTHEME\n\n\n\n\nWEEK 1\nAn Introduction to Remote Sensing\n\n\nWEEK 2\nPortfolio tools: Xaringan and Quarto\n\n\nWEEK 3\nRemote sensing data\n\n\nWEEK 4\nPolicy applications\n\n\nWEEK 5\nAn introduction to Google Earth Engine\n\n\nWEEK 6\nClassification\n\n\nWEEK 7\nClassification the big questions and accuracy\n\n\nWEEK 8\nTemperature and policy"
  },
  {
    "objectID": "WEEK1.html",
    "href": "WEEK1.html",
    "title": "WEEK 1",
    "section": "",
    "text": "This is a learning diary of CASA0023 WEEK 1, the lecture presentation is here, and the practical material is here."
  },
  {
    "objectID": "WEEK1.html#summary",
    "href": "WEEK1.html#summary",
    "title": "WEEK 1",
    "section": "1 Summary",
    "text": "1 Summary\n\n\n\nMindmap of Week 1 Leacture\n\n\n\n\n1.1 Remote sensing\n\nDefinition\nNASA defines remote sensing as acquiring information from a distance, interchangeable used with Earth Observation or EO.\n\n\nData acquisition\nThis is achieved through sensors mounted on a platform, e.g. satellites, planes (aerial imagery), drones, phones, free standing on the ground or sea (with hand held devices), there are more than 150 satellites in orbit carrying sensors.\n\n\nAdvantages\n\nMass of data: satellites collect data on the same points on Earth every day to every 16 days\nFrequency of update and less reliance on authorities (e.g. London Atlas)\nMore free resources to process large volumes of data (e.g. Google Earth Engine)\n\n\n\nTypes of sensor\nPassive sensor\n\nUse energy that is available\nDon’t emit anything\nUsually detecting reflected energy from the sun\nEnergy is in electromagnetic waves…\nSuch as: Human eye, camera, satellite sensor\n\nActive sensor\n\nHave an energy source for illumination\nActively emits electormagentic waves and then waits to receive\nSuch as: Radar, X-ray, LiDAR\n\n\n\n\n1.2 Electromagentic waves\n\n\n\n\n\n\nTerms\n\n\n\n\nWaves of an electromagnetic field, travel through space and carry radiant energy = Electromagnetic radiation (EMR). Waves are part of the EMR spectrum.\nEnergy carried by EMR waves = radiant energy\nEnergy per unit of time = radiant flux\nEnergy from the sun = incoming short wave radiation or shortwave radiation\nEnergy (solar power) from the sun per unit area per unit time (from electromagnetic radiation) = solar irradiance (per unit time - flux)\nEnergy leaving a surface per unit area per unit time = Exitance (emittance) (per unit time - flux)\nFlux means time here.\n\n\n\n\nElectromagnetic radiation (EMR)\nEMR has both electric and magnetic fields, propagates (moves) as waves: c = vλ\n\nc = velocity of light 3 x 10^8 meters per second\nv = frequency, rate of oscillation\nλ = wavelength, distance between two crests\n\nEMR isn’t automatically reflected. It experiences a number of changes prior to hitting the sensor\n\nSurface: Energy being absorbed by the surface and being transmitted through the surface\nAtmospheric: Energy can be scattered by particles in the atmosphere\n\n\n\n\n1.3 Interacting with Earth’s surface\n\nAtmospheric scattering\n\nRayleigh = particles are very small compared to the wavelength\nMie = particles are the same size compared to the wavelength\nNon selective = particles are much larger than the wavelength\n\n\n\nSynthetic Aperture Radar (SAR)\n\nRadar collects at longer wavelengths than optical sensors - pass through clouds that have smaller particle sizes (wavelength dictates how far it can penetrate into medium)\nHas it’s own bands - e.g. P, L, S, C, X, Ku, K\nCollects data at night\n\n\n\nBidirectional Reflectance Distribution Function (BRDF)\n\nView (e.g. sensor) and illumination (e.g. sun) angles can change\nEnergy being reflected from the surface that is smooth or diffuse\n\n\n\nPolarization\nDefinition\nApplicable to Radar: Electromagnetic waves are polarized and the direction depends on the oscillation of the electromagnetic field. When they are reflected from the surface the waves can be linked to surface properties - roughness, shape, orientation, moisture, salinity, density.\nDifferent ploarizations\n\nSingle polarization: same polarization transmitted and received = 1 horizontal (or vertical)\nDual polarization: One sent, different one received = transmits and receives both horizontal and vertical\nQuad polarization: system can transmit and receive four types = emitted in horizontal (H) and received in horizontal (H)\n\n\n\n\n1.4 Remote sensing data\n\nData formats\n\nband interleaved by line (BIL)\nband sequential (BSQ)\nband interleaved by pixel (BIP)\nGeoTIFF (most common)\n\n\n\nFour resolutions\n\nSpatial = the size of the raster grid per pixel (e.g. 20cm or 30m)\nSpectral = the number of bands it records data in…more soon\nTemporal = the time it revisits (e.g. daily, every 7 days, on demand)\nRadiometric = identify differences in light or reflectance, in practice this is the range of possible values.\n\n\n\n\nSpectral resolution. Source: NASA Science\n\n\n\n\nType of orbit\n\ngeosynchronous orbit (GSO) = satellite matches the Earth’s rotation\ngeostationary orbit = holds same position, usually only for communications but some sensors are geostationary."
  },
  {
    "objectID": "WEEK1.html#application",
    "href": "WEEK1.html#application",
    "title": "WEEK 1",
    "section": "2 Application",
    "text": "2 Application\n\nThis week’s lecture is mainly about the basic knowledge of remote sensing and Electromagnetic radiation (EMR), so I would like to introduce some of the remote sensing applications based on spectral characteristics.\n\n\n2.1 Remote sensing applications based on spectral features\nRemote sensing uses spectral features to identify, classify and analyse a variety of features on the surface or in the atmosphere. Remote sensing has many applications using spectral features：\n\nIn the field of agriculture, the spectral characteristics of vegetation can be used to monitor the growth of crops, damage, yield prediction, etc.\nIn the field of environmental, the spectral characteristics of water bodies, soil and atmosphere can be used to monitor water quality, soil types, pollutant types and concentrations, etc.\nIn the field of geology, the spectral characteristics of rocks, minerals, etc. can be used to detect mineral resources, geological formations, seismic activity, etc.\nIn the field of urban planning, the spectral characteristics of buildings, roads, etc. can be used to extract urban spatial information, assess the level of urban development and influencing factors, etc.\n\nThe specific application methods of remote sensing based on spectral features in the field of urban planning are mainly as follows:\n\nUsing remote sensing images to obtain information on the current situation of land use, analyse the structure and spatial distribution characteristics of urban land use, and provide basic data for urban planning.\nUsing remote sensing images for urban ecological environment evaluation, monitoring urban heat island effect, air quality, tree health, water quality and other environmental indicators, and providing ecological guarantee for urban planning.\nUse remote sensing imagery for urban construction change monitoring, identifying the impact of urban construction activities on land use, and providing dynamic management for urban planning.\n\n\n\n2.2 Application case\n\n\n\n\n\n\nSourse\n\n\n\nCharacterizing and classifying urban tree species using bi-monthly terrestrial hyperspectral images in Hong Kong , Sourse: Abbas et al. (2021)\n\n\nUrban trees exhibit a wide range of ecosystem services that have long been unveiled and increasingly reported. The ability to map tree species and analyze tree health conditions would become vividly essential. Remote sensing techniques, especially hyperspectral imaging, are being evolved for species identification and vegetation monitoring from spectral reponse patterns.\n\n\n\nAn example of image clustering and corresponding spectral signatures of classes. The shadow class in grey represents canopy shadow and/or branches, and the shadow class in orange indicates shaded leaves.\n\n\nIn this study, a hyperspectral library for urban tree species in Hong Kong was established comprising 75 urban trees belonging to 19 species. 450 bi-monthly images were acquired by a terrestrial hyperspectral camera (SPECIM-IQ) from November 2018 to October 2019. A Deep Neural Network classification model was developed to identify tree species from the hyperspectral imagery with an overall accuracy ranging from 85% to 96% among different seasons. Representative spectral reflectance curves of healthy and unhealthy conditions for each species were extracted and analyzed. This can be used to identify urban trees and monitor their health.\n\n\n\nThe overall workflow of species classification framework using the Deep Neural Network modelling."
  },
  {
    "objectID": "WEEK1.html#reflection",
    "href": "WEEK1.html#reflection",
    "title": "WEEK 1",
    "section": "3 Reflection",
    "text": "3 Reflection\nDuring this week I have learnt about the basics of remote sensing and Electromagnetic radiation (EMR), which has greatly broadened my horizons when researching urban. Due to limited data collection facilities, detailed urban datasets (e.g. traffic flow data, mobile phone signalling data, etc.) are only available for main cities in developed areas, which makes many studies not reproducible in a wide range of non-developed or small cities. However, the extensive coverage of remote sensing data compensates well for this shortcoming, and the variety of data collected through the rich diversity of sensors can be of great help in urban (or regional) analysis.\n\n\n\n\nAbbas, Sawaid, Qian Peng, Man Sing Wong, Zhilin Li, Jicheng Wang, Kathy Tze Kwun Ng, Coco Yin Tung Kwok, and Karena Ka Wai Hui. 2021. “Characterizing and Classifying Urban Tree Species Using Bi-Monthly Terrestrial Hyperspectral Images in Hong Kong.” ISPRS Journal of Photogrammetry and Remote Sensing 177 (July): 204–16. https://doi.org/10.1016/j.isprsjprs.2021.05.003."
  },
  {
    "objectID": "WEEK2.html",
    "href": "WEEK2.html",
    "title": "WEEK 2",
    "section": "",
    "text": "Homework: Introduction to Hyperspectral Radiometer\nThis website and this slide can be used as a practical presentation of the content of this week."
  },
  {
    "objectID": "WEEK3.html",
    "href": "WEEK3.html",
    "title": "WEEK 3",
    "section": "",
    "text": "This is a learning diary of CASA0023 WEEK 3, the lecture presentation is here, and the practical material is here."
  },
  {
    "objectID": "WEEK3.html#summary",
    "href": "WEEK3.html#summary",
    "title": "WEEK 3",
    "section": "1 Summary",
    "text": "1 Summary\n\n\n\nMindmap of Week 3 Leacture\n\n\n\n\n1.1 Pre-knowledge\n\nDifferent sensors\n\nMSS (Multispectral Scanner)\nRBV (Return Beam Vidicon Camera)\n\n\n\nPush broom vs Whisk broom\n\nWhisk broom or spotlight or across track scanners: Mirror reflects light onto 1 detector - Landsat\nPush broom or along track scanners: several detectors that are pushed along - SPOT, Quickbird\n\n\n\n\n1.2 Corrections\n\nRegression\n\\[\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n\\]\n\nβ0 is the intercept (the value of y when x = 0)\nβ1 the ‘slope’ the change in the value of y for a 1 unit change in the value of x\nϵi is a random error term (positive or negative)- if you add all of the vertical differences between the blue line and all of the residuals, it should sum to 0\nAny value of y along the blue line can be modeled using the corresponding value of x\n\n\n\nGeometric correction\nWhat leads to image distortions\n\nView angle (off-nadir)\nTopography (e.g. hills not flat ground)\nWind (if from a plane)\nRotation of the earth (from satellite)\n\nGeometric correction solution\n\n\n\nGeometric correction. Source: Abdul Basith\n\n\n\nIdentify Ground Control Points (GPS) to match known points in the image and a reference dataset\nTake the coordinates and model them to give geometric transformation coefficients, linear regression with our distorted x or y as the dependent or independent\n\nInput to output (forward mapping): the issue with this is that we are modelling the rectified x and y which could fall anywhere on the gold standard map (e.g. not on a grid square or at a floating point)\nOutput to input (backward mapping): for every value in the output (gold standard) pixel we can get a value in the original input image. The images are distorted as so might not completely overlap. The goal is to match the distorted image with the gold standard image, so we want the pixels to line up\n\nPlot these and try to minimise the RMSE - Jensen sets a RMSE value of 0.5, typically might add more GCPs to reduce the RMSE (The model with the lowest RMSE will fit best)\n\nRMSE: (observed - predicted (the residual))^2, sum them and divide by number of data points, square root that total\nResample methods: Nearest Neighbor, Linear, Cubic, Cubic spline\n\n\n\n\nAtmospheric correction\nNecessary and unnecessary atmospheric correction\n\nNecessary\n\nBiophysical parameters needed (e.g. temperature, leaf area index, NDVI)\nUsing spectral signatures through time and space\n\nUnnecessary\n\nClassification of a single image\nIndependent classification of multi date imagery\nComposite images (combining images)\nSingle dates or where training data extracted from all data\n\n\nAtmospheric correction in action\n\n\n\nAtmospheric correction examples of three scenes (Bands 1, 2, and 3). Source: Liang et al. 2001\n\n\n\nAbsorption and scattering create the haze = reduces contrast of image\nScattering = can create the “adjacency effect”, radiance from pixels nearby mixed into pixel of interest\n\nAtmospheric correction types\n\nRelative (to something)\n\nNormalize\n\nNormalize intensities of different bands within a single image\nNormalise intensities of bands from many dates to one date\n\nDark object subtraction (DOS) or histogram adjustment\n\nSearches each band for the darkest value then subtracts that from each pixel\nLandsat bands 1-3 (visible) have increased scattering vs longer wavelengths\n\nPsuedo-invariant Features (PIFs)\n\nAssume brightness pixels linearly related to a base image\nRegression per band\nAdjust the image based on the regression result\nHere y is the value of our base. To get y we multiply our new date pixel (x) by the coefficient and add the intercept value\nApply this to the rest of the pixels\n\n\nAbsolute (definitive)\n\nMethod\n\nChange digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet\nWe do this through atmospheric radiative transfer models and there are many to select from\nHowever, nearly all assume atmospheric measurements are available which are used to “invert” the image radiance to scaled surface reflectance\nThe scattering and absorption information comes from atmopshierc radiative transfer code such as MODTRAN 4+ and the Second Simulation of the Satellite Signal in the Solar Spectrum (6S), which can now be used through python - called Py6S\n\nAbsolute Data requirements\n\nAn atmopsheric model (summer, tropical): usually you can select from the tool\nLocal atmopsheric visibility: from a weather station, like airports\nImage altitude\n\nAbsolute Tools\n\nACORN: Atmopsehic CORection Now\nFLAASH: Fast Line of-sight Atmopsheric Analysis\nQUAC: Quick Atmopsheric Correction\nATCOR: The ATmospheric CORrection program\nSMAC: Simplified Model for Atmospheric Correction (SMAC)\n\n\nEmpirical Line Correction\n\nWe can go and take measurements in situ using a field spectrometer, this does require measurements at the same time as the satellite overpass\nThen use these measurements in linear regression against the satellite data raw digital number\n\n\n\n\nOrthorectification / Topographic correction\n\n\n\nA view captured from an oblique angle (for example, 25°, left) must be corrected for relief displacement caused by terrain to generate the orthorectified view (looking straight down, right). Orthoimagery is produced by calculating the nadir view for every pixel. Source: Esri Insider, 2016\n\n\nA subset of georectification\n\nGeorectification = giving coordinates to an image\nOrthorectification = removing distortions… making the pixels viewed at nadir (straight down)\n\nRequires\n\nSensor geometry\nAn elevation model\n\nSoftware / formulas to do this\n\nJensen covers the following formulas: Cosine correction, Minnaert correction, Statistical Empirical correction, C Correction (advancing the Cosine)\nSoftware: QGIS, SAGA GIS, R package topocorr, R package RStoolbox\n\nSolar location\n\nSolar azimuth = compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset\nSolar zenith = angle of local zenith (above the point on ground) and sun from vertical (90° - elevation)\n\n\n\nRadiometric calibration\n\nSensors capture image brightness and distributed as a Digital Number (or DN) - allows for efficient storage but has no units\nSpectral radiance is the amount of light within a band from a sensor in the field of view (FOV), it is independent of the sensor, measured in Watts (power or light here)\nDN to spectral radiance = radiometric calibration\nSensor calibration = the relationship between Gain and Bias are usually provided but we can calcaulte them\n\n\n\n\n\n\n\nTerms\n\n\n\n\nRadiance: refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor\n\nMight also be called Top of Atmosphere (TOA) radiance\nHow much light the instrument sees in meaningful units but still have effects of: Light source, atmosphere and surface material\nWe can remove the effects of the light source to generate Top of Atmosphere reflectance but usually this is combined within the radiance to reflectance step\n\nIrradiance: is used to describe downwelling radiation reaching the Earth from the sun\nDigital number (DN)\n\nIntensity of the electromagnetic radiation per pixel\nPixel values that aren’t calibrated and have no unit\nHave light source\nEffects of sensor + atmosphere + material\nValues range from 0 - 255 (Lansat 5) = 8 bit or 0 - 65536 Landsat 8 (12 bit)\n\nReflectance: We need to account for atmospheric and illumination effects to create reflectance. BUT this typically doesn’t deal with shadows and directional effects (e.g. viewing angles) = apparent reflectance However, this is often called reflectance\n\nReflectance is a property of a material (e.g. reflectance of grass is a property of grass)\nThe issue with radiance is that is contains physical properties AND is dependent on the light source\n\nHemispherical reflectance: all of the light leaving the surface goes straight to the sensor (nothing is intercepted or at an angle)\nPath radiance: radiance reflected above the surface (e.g. scattering)\nAtmospheric attenuation: absorption of EMR due to materials in atmosphere (e.g. water vapour)\nLocal: specific to pixel\n`Neighbourhood: pixels within a range (nearby)\n\n\n\n\n\n\n1.3 Data joining and enhancement\n\nJoining data sets\n\nThis is termed “Mosaicking” in remote sensing - but it’s not much different to merging in GIS\nIn Remote Sensing we usually feather the images together\nThis creates a seamless mosaic or image(s)\nThe dividing line is termed the seamline\nWe have a base image and “other” or second image\n\n\n\nHow to join data sets\n\nStandardization (dividing the SR value by a maximum value per band) and normalization (divide the standarised value by the sum of values across all bands) applied to each image\nUndertake further relative radiometric normalization\nClassify each image alone\nCalculate other metrics from the image\n\n\n\nImage enhancement\nContrast enhancement\n\n\n\nContrast enhancement in QGIS. Source: Atilo Francois\n\n\n\nMinimum - Maximum\nPercentage Linear and Standard Deviation\nPiecewise Linear Contrast Stretch\n\nRatio\n\nBand ratioing means dividing the pixels in one band by the corresponding pixels in a second band.\nExample: Normalized Burn Ratio = (NIR - SWIR) / (NIR + SWIR)\n\nIn Landsat 4-7, NBR = (Band 4 – Band 7) / (Band 4 + Band 7)\nIn Landsat 8-9, NBR = (Band 5 – Band 7) / (Band 5 + Band 7)\n\n\nFiltering\n\nLow pass or low frequency (averages the surrounding pixels)\nHigh pass or high frequency - enhance local variations\nEdge enhancement\n\nPCA (Principal Component)\n\nTransform multi-spectral data into uncorrelated and smaller dataset\nHas most of the original information\nReduces future computation “dimensionatliy reduction”\nThe first component will capture most of the variance within the dataset\nIn R this is from the RStoolbox packagerasterPCA()\nPCA example, multi-date PCA - bands from both time points are combined into one image, then PCA\n\nTexture\n\nImages just use tonal (spectral) data not texture\nTexture: spatial variation of gray values\nFirst order (occurrence): use counts or occurrences\nSecond order(co-occurrence): relationship between pixel pairs “a function of both the angular relationship and distance between two (or kernel) neighboring pixels”\n\nFusion\nImage fusion is where data from multiple sensors / sources is fused together\n\nPan sharpen\nData Fusion"
  },
  {
    "objectID": "WEEK3.html#application",
    "href": "WEEK3.html#application",
    "title": "WEEK 3",
    "section": "2 Application",
    "text": "2 Application\n\nThis week, the main focus was on the correction, joining and enhancement of remote sensing images, mostly applied to pre-processing before analysis, but remote sensing image enhancement has a wider range of applications.\n\n\n2.1 Applications of remote sensing image enhancement\nRemote sensing image enhancement refers to the improvement of the quality and information of remote sensing images through some technical means to make them more suitable for human vision or subsequent analytical processing. Remote sensing image enhancement can be applied to the following areas:\nContrast enhancement: By adjusting the grey level of a remotely sensing image, the contrast of the image is increased to make it clearer and brighter. Contrast enhancement can be done using methods such as histogram stretching, histogram equalisation and segmented linear stretching.\nBand ratioing: Extracts specific information from remote sensing images, such as vegetation indices, water indices, soil indices, etc., by calculating the ratio between different bands. Band ratios can be calculated using methods such as band arithmetic and band combination.\nFiltering: Remove noise or enhance edges and textures in an image by convolving the remote sensing image in the spatial or frequency domain. Filtering can use methods such as smoothing filtering, sharpening filtering and edge detection filtering.\nPrincipal component analysis (PCA): By applying orthogonal transformations to multiple bands of remote sensing images, the main information in the image is extracted and the redundancy and correlation of the data is reduced. Principal component analysis can use statistical methods or methods such as wavelet transform.\nTexture: Describes the surface roughness or structural features in an image by calculating the grey scale variation within a region of the remotely sensed image. Texturing can use methods such as grey level co-occurrence matrix, grey level distance matrix, grey level gradient matrix, etc.\nFusion: Increasing the spatial or spectral resolution of an image and increasing the amount of information in the image by combining remote sensing images from different sources or at different resolutions. Fusion can be done using methods such as HIS transform, wavelet transform, multi-resolution analysis, etc.\n\n\n2.2 Application case\n\n\n\n\n\n\nSourse\n\n\n\nPCA‐based land‐use change detection and analysis using multitemporal and multisensor satellite data , Sourse: Deng et al. (2008)\n\n\nRemote‐sensing change detection based on multitemporal, multispectral, and multisensor imagery has been developed over several decades and provided timely and comprehensive information for planning and decision‐making. In practice, however, it is still difficult to select a suitable change‐detection method, especially in urban areas, because of the impacts of complex factors.\nThis paper presents a new method using multitemporal and multisensor data (SPOT‐5 and Landsat data) to detect land‐use changes in an urban environment based on principal‐component analysis (PCA) and hybrid classification methods. After geometric correction and radiometric normalization, PCA was used to enhance the change information from stacked multisensor data. Then, a hybrid classifier combining unsupervised and supervised classification was performed to identify and quantify land‐use changes. Finally, stratified random and user‐defined plots sampling methods were synthetically used to obtain total 966 reference points for accuracy assessment.\n\n\n\nExample of land‐use changes from cropland to urban land (Economic and Technological Development Zone). (a) Pan image of ETM (2000); (b) aerial photograph (2000); (c) RGB composition image of SPOT‐5 (2003); (d) RGB composition image of IKONOS (2003); (e)–(h) first four principal components.\n\n\nAlthough errors and confusion exist, this method shows satisfying results with an overall accuracy to be 89.54% and 0.88 for the kappa coefficient. When compared with the post‐classification method, PCA‐based change detection also showed a better accuracy in terms of overall, producer’s, and user’s accuracy and kappa index. The results suggested that significant land‐use changes have occurred in Hangzhou City from 2000 to 2003, which may be related to rapid economy development and urban expansion. It is further indicated that most changes occurred in cropland areas due to urban encroachment.\n\n\n\nLand use and land‐use change detected using the PCA‐based approach."
  },
  {
    "objectID": "WEEK3.html#reflection",
    "href": "WEEK3.html#reflection",
    "title": "WEEK 3",
    "section": "3 Reflection",
    "text": "3 Reflection\nThis week I learnt the principles and methods of correcting, joining and enhancing remotely sensed images and understood how to pre-process remotely sensed data. Although most of the data we acquired had already been pre-processed, this knowledge gave me a better understanding of how to apply remote sensing images to the actual analysis and what could cause errors and whether further processing was needed. In addition, the study of remote sensing image enhancement has taught me more about remote sensing data analysis methods and principles in a deeper way.\n\n\n\n\nDeng, J. S., K. Wang, Y. H. Deng, and G. J. Qi. 2008. “PCA‐based Land‐use Change Detection and Analysis Using Multitemporal and Multisensor Satellite Data.” International Journal of Remote Sensing 29 (16): 4823–38. https://doi.org/10.1080/01431160801950162."
  },
  {
    "objectID": "WEEK4.html",
    "href": "WEEK4.html",
    "title": "WEEK 4",
    "section": "",
    "text": "This is a learning diary of CASA0023 WEEK 4, the lecture presentation is here, and the practical material is here."
  },
  {
    "objectID": "WEEK5.html",
    "href": "WEEK5.html",
    "title": "WEEK 5",
    "section": "",
    "text": "This is a learning diary of CASA0023 WEEK 5, the lecture presentation is here, and the practical material is here."
  },
  {
    "objectID": "WEEK6.html",
    "href": "WEEK6.html",
    "title": "WEEK 6",
    "section": "",
    "text": "This is a learning diary of CASA0023 WEEK 6, the lecture presentation is here, and the practical material is here."
  },
  {
    "objectID": "WEEK7.html",
    "href": "WEEK7.html",
    "title": "WEEK 7",
    "section": "",
    "text": "This is a learning diary of CASA0023 WEEK 7, the lecture presentation is here, and the practical material is here."
  },
  {
    "objectID": "WEEK8.html",
    "href": "WEEK8.html",
    "title": "WEEK 8",
    "section": "",
    "text": "This is a learning diary of CASA0023 WEEK 8, the lecture presentation is here, and the practical material is here."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbas, Sawaid, Qian Peng, Man Sing Wong, Zhilin Li, Jicheng Wang, Kathy\nTze Kwun Ng, Coco Yin Tung Kwok, and Karena Ka Wai Hui. 2021.\n“Characterizing and Classifying Urban Tree Species Using\nBi-Monthly Terrestrial Hyperspectral Images in Hong\nKong.” ISPRS Journal of Photogrammetry and\nRemote Sensing 177 (July): 204–16. https://doi.org/10.1016/j.isprsjprs.2021.05.003.\n\n\nDeng, J. S., K. Wang, Y. H. Deng, and G. J. Qi. 2008.\n“PCA‐based Land‐use Change Detection and Analysis\nUsing Multitemporal and Multisensor Satellite Data.”\nInternational Journal of Remote Sensing 29 (16): 4823–38. https://doi.org/10.1080/01431160801950162."
  }
]